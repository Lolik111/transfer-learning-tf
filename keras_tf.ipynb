{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"keras_tf.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"euztsoNv4aEI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Install the PyDrive wrapper & import libraries.\n","# This only needs to be done once per notebook.\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","\n","import tensorflow as tf\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.python.keras.applications import xception\n","from google.colab import files"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wbQhhJRElbOR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","\n","# Download a file based on its file ID.\n","#\n","files = ['10bcf4_bV-B_GmX_40BDjD7BZjRve-ciw',\n","'1gdTW4bRF9reGffYwgQoKSfqMjjXoWKNq',\n","'1-4VuiQHr0mCu8IK28ACPYDxpouxKUNvo',\n","'19meBzmNDy570ch88u812KuEFXsyt4yPc',\n","'1Z8bY6e-LWdVd6y-TP3qEe75AURytvGxY',\n","'1-PpJOwSCbpleYD0Ss81Sm6k-mQvpQx7L',\n","'1PsUeYCptA1RTpSvQAk7vidPIy5qX0KTN',\n","'1Qvl-jozmmU2YqOt395Beuuph4ivYJb0T',\n","'1QZjABkrELGxL7fwYXXD0gKammp86bhhn',\n","'1ePsQIhQUMPFC7VdFzOhIY0Uf5fZO2igU',\n","'1RniG4-dOuyBCmTRjfSCH4XWblGs4_wLW',\n","'1_MqKcZQqzvIWTo3A1y5V54G2ewcDhbEY']\n","for i in range(12):\n","    file_id = files[0]\n","    downloaded = drive.CreateFile({'id': file_id})\n","    downloaded.GetContentFile('train-' + str(i))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"15x4vmpm4nOG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":2}],"base_uri":"https://localhost:8080/","height":72},"outputId":"46fec8e5-ff3e-4503-cfb0-cd034154b501","executionInfo":{"status":"ok","timestamp":1522312832634,"user_tz":-180,"elapsed":5492,"user":{"displayName":"Рамиль Г","photoUrl":"//lh4.googleusercontent.com/-8aSyWSiuugc/AAAAAAAAAAI/AAAAAAAACj0/jvk5E13FNMY/s50-c-k-no/photo.jpg","userId":"115296759250209017991"}}},"cell_type":"code","source":["base_model = xception.Xception(include_top=False, weights='imagenet', input_shape=(299, 299,3))\n","x = base_model.output\n","\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","\n","\n","predictions = Dense(196, activation='softmax')(x)\n","\n","# this is the model we will train\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# for layer in base_model.layers:\n","#     layer.trainable = False\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","run_config = tf.estimator.RunConfig()\n","run_config = run_config.replace(keep_checkpoint_max=25, save_summary_steps=10)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","est = tf.keras.estimator.model_to_estimator(model, model_dir='out', config=run_config)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using the Keras model from memory.\n","INFO:tensorflow:Using config: {'_model_dir': 'out', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 25, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb2c9c5b780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"}]},{"metadata":{"id":"Xx58EdW-qYXO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["DATASET_FILE = 'cars_annos'\n","seed = 123\n","n_threads = 2\n","image_size=299\n","BATCH_SIZE=32"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oJTidzzgqYeB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def parse_example_proto(example_serialized):\n","    # Dense features in Example proto.\n","    feature_map = {\n","        'image/encoded': tf.FixedLenFeature([], dtype=tf.string,\n","                                            default_value=''),\n","        'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64,\n","                                                default_value=-1),\n","        'image/class/text': tf.FixedLenFeature([], dtype=tf.string,\n","                                               default_value=''),\n","        'image/height': tf.FixedLenFeature([1], dtype=tf.int64,\n","                                                default_value=-1),\n","        'image/width': tf.FixedLenFeature([1], dtype=tf.int64,\n","                                                default_value=-1),\n","    }\n","    sparse_float32 = tf.VarLenFeature(dtype=tf.float32)\n","    # Sparse features in Example proto.\n","    feature_map.update(\n","        {k: sparse_float32 for k in ['image/object/bbox/xmin',\n","                                     'image/object/bbox/ymin',\n","                                     'image/object/bbox/xmax',\n","                                     'image/object/bbox/ymax']})\n","\n","    features = tf.parse_single_example(example_serialized, feature_map)\n","    label = tf.cast(features['image/class/label'], dtype=tf.int32)\n","    label = tf.subtract(label, 1)\n","    xmin = tf.cast(tf.reshape(features['image/object/bbox/xmin'].values, shape=[]), dtype=tf.int32)\n","    ymin = tf.cast(tf.reshape(features['image/object/bbox/ymin'].values, shape=[]), dtype=tf.int32)\n","    xmax = tf.cast(tf.reshape(features['image/object/bbox/xmax'].values, shape=[]), dtype=tf.int32)\n","    ymax = tf.cast(tf.reshape(features['image/object/bbox/ymax'].values, shape=[]), dtype=tf.int32)\n","    \n","    tg_y = tf.subtract(ymax, ymin)\n","    tg_x = tf.subtract(xmax, xmin)\n","\n","    return features['image/encoded'], label, xmin, ymin, tg_x, tg_y, features['image/class/text']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XrcJalX_4nfm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def decode_jpeg(image_buffer, scope=None):\n","    \"\"\"Decode a JPEG string into one 3-D float image Tensor.\n","\n","    Args:\n","      image_buffer: scalar string Tensor.\n","      scope: Optional scope for name_scope.\n","    Returns:\n","      3-D float Tensor with values ranging from [0, 1).\n","    \"\"\"\n","    with tf.name_scope(values=[image_buffer], name=scope,\n","                       default_name='decode_jpeg'):\n","        # Decode the string as an RGB JPEG.\n","        # Note that the resulting image contains an unknown height and width\n","        # that is set dynamically by decode_jpeg. In other words, the height\n","        # and width of image is unknown at compile-time.\n","        image = tf.image.decode_jpeg(image_buffer, channels=3)\n","\n","        # After this point, all image pixels reside in [0,1)\n","        # until the very end, when they're rescaled to (-1, 1).  The various\n","        # adjust_* ops all require this range for dtype float.\n","        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n","        return image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_7mg_juZsj8p","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def apply_with_random_selector(x, func, num_cases):\n","    \"\"\"Computes func(x, sel), with sel sampled from [0...num_cases-1].\n","\n","    Args:\n","      x: input Tensor.\n","      func: Python function to apply.\n","      num_cases: Python int32, number of cases to sample sel from.\n","\n","    Returns:\n","      The result of func(x, sel), where func receives the value of the\n","      selector as a python integer, but sel is sampled dynamically.\n","    \"\"\"\n","    \n","    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n","    # Pass the real x only to one of the func calls.\n","    return control_flow_ops.merge([\n","        func(tf.python.ops.control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n","        for case in range(num_cases)])[0]\n","\n","def image_preprocessing(image_buffer, xmin, ymin, tg_x, tg_y, train=True):\n","#     if bbox is None:\n","#         raise ValueError('Please supply a bounding box.')\n","\n","    image = decode_jpeg(image_buffer)\n","\n","    if train:\n","#         distorted_bbox = tf.image.sample_distorted_bounding_box(tf.shape(image), bounding_boxes=bbox, min_object_covered=.8,\n","#                                                aspect_ratio_range=(0.05, 1 / ratio), area_range=(0.05, 1.0), max_attempts=100)\n","#         bbox_begin, bbox_size, distort_bbox = distorted_bbox\n","        \n","#         distorted_image = tf.slice(image, bbox_begin, bbox_size)\n","        distorted_image = tf.image.crop_to_bounding_box(image, ymin, xmin, tg_y, tg_x)\n","#         rnd_val = tf.random_uniform([], maxval=4, dtype=tf.int32)\n","        distorted_image = tf.image.resize_images(distorted_image, [image_size, image_size],\n","                                                 method=0)\n","        distorted_image.set_shape([image_size, image_size, 3])\n","        distorted_image = tf.image.random_flip_left_right(distorted_image)\n","        \n","    else:\n","#         image = eval_image(image, height, width)\n","        pass\n","\n","    return distorted_image\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6HiPozi6skBF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def parse_fn(example):\n","    image, label, xmin, ymin, tg_x, tg_y, _ = parse_example_proto(example)\n","    image = image_preprocessing(image, xmin, ymin, tg_x, tg_y)\n","    return {'input_1':image}, label\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"8CQJdnAUskZp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def input_fn():\n","    with tf.device('/cpu:0'):\n","        files = tf.data.Dataset.list_files(\"./train-*\")\n","        dataset = files.interleave(tf.data.TFRecordDataset, 1)\n","        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1150))\n","        dataset = dataset.map(map_func=parse_fn, num_parallel_calls=n_threads)\n","        dataset = dataset.batch(batch_size=BATCH_SIZE)\n","        dataset = dataset.prefetch(buffer_size=BATCH_SIZE)\n","        iterator = dataset.make_one_shot_iterator()\n","        batch_features, batch_labels = iterator.get_next()\n","\n","    return batch_features, batch_labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nq278s_t3Qcg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":18}],"base_uri":"https://localhost:8080/","height":416},"outputId":"78f1ab4f-c377-4e0c-ff29-64beabadccab"},"cell_type":"code","source":["with tf.device('/gpu:0'):\n","    est.train(input_fn=input_fn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from out/model.ckpt-1964\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 1965 into out/model.ckpt.\n","INFO:tensorflow:loss = 99442.31, step = 1965\n","INFO:tensorflow:global_step/sec: 0.4581\n","INFO:tensorflow:loss = 102060.92, step = 2065 (218.293 sec)\n","INFO:tensorflow:global_step/sec: 0.46884\n","INFO:tensorflow:loss = 102384.21, step = 2165 (213.292 sec)\n","INFO:tensorflow:Saving checkpoints for 2243 into out/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.464338\n","INFO:tensorflow:loss = 100897.11, step = 2265 (215.361 sec)\n","INFO:tensorflow:global_step/sec: 0.46861\n","INFO:tensorflow:loss = 127147.81, step = 2365 (213.397 sec)\n","INFO:tensorflow:global_step/sec: 0.468523\n","INFO:tensorflow:loss = 110272.34, step = 2465 (213.436 sec)\n","INFO:tensorflow:Saving checkpoints for 2524 into out/model.ckpt.\n","INFO:tensorflow:global_step/sec: 0.463898\n","INFO:tensorflow:loss = 104938.16, step = 2565 (215.565 sec)\n"],"name":"stdout"}]},{"metadata":{"id":"9JhOinfzgiYK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"a3c3bd75-a22f-484f-8fa5-94e21221ac85","executionInfo":{"status":"ok","timestamp":1522312774276,"user_tz":-180,"elapsed":608,"user":{"displayName":"Рамиль Г","photoUrl":"//lh4.googleusercontent.com/-8aSyWSiuugc/AAAAAAAAAAI/AAAAAAAACj0/jvk5E13FNMY/s50-c-k-no/photo.jpg","userId":"115296759250209017991"}}},"cell_type":"code","source":["est.latest_checkpoint()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'out/model.ckpt-1964'"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"43BF7lFb4dyZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[]}]}